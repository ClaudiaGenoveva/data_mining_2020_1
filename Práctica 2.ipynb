{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vagrant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download()\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np    \n",
    "from numpy import dot\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preface Automated Machine Learning Methods, Systems, Challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Párrafo 1\n",
    "text_01 = \"I have been very passionate about automating machine learning myself ever since our Automatic Statistician project started back in 2014. I want us to be really ambitious in this endeavor; we should try to automate all aspects of the entire machine learning and data analysis pipeline. This includes automating data collection and experiment design; automating data cleanup and missing data imputa- tion; automating feature selection and transformation; automating model discovery, criticism, and explanation; automating the allocation of computational resources; automating hyperparameter optimization; automating inference; and automating model monitoring and anomaly detection. This is a huge list of things, and we’d optimally like to automate all of it.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Párrafo 2\n",
    "text_02 = \"There is a caveat of course. While full automation can motivate scientific research and provide a long-term engineering goal, in practice, we probably want to semiautomate most of these and gradually remove the human in the loop as needed. Along the way, what is going to happen if we try to do all this automation is that we are likely to develop powerful tools that will help make the practice of machine learning, first of all, more systematic (since it’s very ad hoc these days) and also more efficient.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Párrafo 3\n",
    "text_03 = \"These are worthy goals even if we did not succeed in the final goal of automation, but as this book demonstrates, current AutoML methods can already surpass human machine learning experts in several tasks. This trend is likely only going to intensify as we’re making progress and as computation becomes ever cheaper, and AutoML is therefore clearly one of the topics that is here to stay. It is a great time to get involved in AutoML, and this book is an excellent starting point.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimento 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://twitter.com/GaeboraKae/status/1169424976413372416\n",
    "text_01 = \"Quisiera entender que le ven de rico o divertido a hacer cebo en el metro, de verdad, acabo de ver a cuatro parejas en la misma área dandolo todo con su queso\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://twitter.com/VICEenEspanol/status/1169705966234943488\n",
    "text_02 = \"Dos mujeres que se tomaban una selfie al lado de una pista de aterrizaje serrana murieron, al parecer, accidentalmente; un piloto aviador que solía viajar a la Sierra Tarahumara y un maestro de artes marciales retirado fueron asesinados en 2017.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://twitter.com/Anaro74/status/1169579828963463168\n",
    "text_03 = \"Buenos días desde un país donde el Subsecretario de Educación dice que el comunismo es necesario para transformar a México; donde se disparan 312% los casos de dengue porque no compraron insecticidas, Mireles llama pirujas a las concubinas y el Presi quiere quitar el INE Café?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'been', 'very', 'passionate', 'about', 'automating', 'machine', 'learning', 'myself', 'ever', 'since', 'our', 'automatic', 'statistician', 'project', 'started', 'back', 'in', '2014', 'i', 'want', 'us', 'to', 'be', 'really', 'ambitious', 'in', 'this', 'endeavor', 'we', 'should', 'try', 'to', 'automate', 'all', 'aspects', 'of', 'the', 'entire', 'machine', 'learning', 'and', 'data', 'analysis', 'pipeline', 'this', 'includes', 'automating', 'data', 'collection', 'and', 'experiment', 'design', 'automating', 'data', 'cleanup', 'and', 'missing', 'data', 'imputa', 'tion', 'automating', 'feature', 'selection', 'and', 'transformation', 'automating', 'model', 'discovery', 'criticism', 'and', 'explanation', 'automating', 'the', 'allocation', 'of', 'computational', 'resources', 'automating', 'hyperparameter', 'optimization', 'automating', 'inference', 'and', 'automating', 'model', 'monitoring', 'and', 'anomaly', 'detection', 'this', 'is', 'huge', 'list', 'of', 'things', 'and', 'we', 'd', 'optimally', 'like', 'to', 'automate', 'all', 'of', 'it']\n"
     ]
    }
   ],
   "source": [
    "text_01_tokens = tokenizer.tokenize(text_01.lower()) #tokenizar y quitar signos de puntuación\n",
    "#print(text_01_tokens)\n",
    "\n",
    "text_01_tokens_wout_stopwords = []\n",
    "\n",
    "for word in text_01_tokens:\n",
    "    if word not in stopwords.words('spanish'): text_01_tokens_wout_stopwords.append(word)\n",
    "\n",
    "print(text_01_tokens_wout_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['there', 'is', 'caveat', 'of', 'course', 'while', 'full', 'automation', 'can', 'motivate', 'scientific', 'research', 'and', 'provide', 'long', 'term', 'engineering', 'goal', 'in', 'practice', 'we', 'probably', 'want', 'to', 'semiautomate', 'most', 'of', 'these', 'and', 'gradually', 'remove', 'the', 'human', 'in', 'the', 'loop', 'as', 'needed', 'along', 'the', 'way', 'what', 'is', 'going', 'to', 'happen', 'if', 'we', 'try', 'to', 'do', 'all', 'this', 'automation', 'is', 'that', 'we', 'are', 'likely', 'to', 'develop', 'powerful', 'tools', 'that', 'will', 'help', 'make', 'the', 'practice', 'of', 'machine', 'learning', 'first', 'of', 'all', 'more', 'systematic', 'since', 'it', 's', 'very', 'ad', 'hoc', 'these', 'days', 'and', 'also', 'more', 'efficient']\n"
     ]
    }
   ],
   "source": [
    "text_02_tokens = tokenizer.tokenize(text_02.lower()) \n",
    "#print(text_02_tokens)\n",
    "\n",
    "text_02_tokens_wout_stopwords = []\n",
    "\n",
    "for word in text_02_tokens:\n",
    "    if word not in stopwords.words('spanish'): text_02_tokens_wout_stopwords.append(word)\n",
    "\n",
    "print(text_02_tokens_wout_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['buenos', 'días', 'país', 'subsecretario', 'educación', 'dice', 'comunismo', 'necesario', 'transformar', 'méxico', 'disparan', '312', 'casos', 'dengue', 'compraron', 'insecticidas', 'mireles', 'llama', 'pirujas', 'concubinas', 'presi', 'quiere', 'quitar', 'ine', 'café']\n"
     ]
    }
   ],
   "source": [
    "text_03_tokens = tokenizer.tokenize(text_03.lower()) \n",
    "#print(text_03_tokens)\n",
    "\n",
    "text_03_tokens_wout_stopwords = []\n",
    "\n",
    "for word in text_03_tokens:\n",
    "    if word not in stopwords.words('spanish'): text_03_tokens_wout_stopwords.append(word)\n",
    "\n",
    "print(text_03_tokens_wout_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n",
      "89\n",
      "25\n",
      "303\n"
     ]
    }
   ],
   "source": [
    "print(len(text_01_tokens_wout_stopwords))\n",
    "print(len(text_02_tokens_wout_stopwords))\n",
    "print(len(text_03_tokens_wout_stopwords))\n",
    "print(len(text_01_tokens_wout_stopwords) + len(text_02_tokens_wout_stopwords) + len(text_01_tokens_wout_stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creación de la BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicc_texts = {\"text_01\": text_01_tokens_wout_stopwords, \n",
    " \"text_02\": text_02_tokens_wout_stopwords, \n",
    " \"text_03\": text_03_tokens_wout_stopwords}\n",
    "#dicc_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'i': 2,\n",
       " 'have': 1,\n",
       " 'been': 1,\n",
       " 'very': 2,\n",
       " 'passionate': 1,\n",
       " 'about': 1,\n",
       " 'automating': 9,\n",
       " 'machine': 3,\n",
       " 'learning': 3,\n",
       " 'myself': 1,\n",
       " 'ever': 1,\n",
       " 'since': 2,\n",
       " 'our': 1,\n",
       " 'automatic': 1,\n",
       " 'statistician': 1,\n",
       " 'project': 1,\n",
       " 'started': 1,\n",
       " 'back': 1,\n",
       " 'in': 4,\n",
       " '2014': 1,\n",
       " 'want': 2,\n",
       " 'us': 1,\n",
       " 'to': 7,\n",
       " 'be': 1,\n",
       " 'really': 1,\n",
       " 'ambitious': 1,\n",
       " 'this': 4,\n",
       " 'endeavor': 1,\n",
       " 'we': 5,\n",
       " 'should': 1,\n",
       " 'try': 2,\n",
       " 'automate': 2,\n",
       " 'all': 4,\n",
       " 'aspects': 1,\n",
       " 'of': 8,\n",
       " 'the': 6,\n",
       " 'entire': 1,\n",
       " 'and': 11,\n",
       " 'data': 4,\n",
       " 'analysis': 1,\n",
       " 'pipeline': 1,\n",
       " 'includes': 1,\n",
       " 'collection': 1,\n",
       " 'experiment': 1,\n",
       " 'design': 1,\n",
       " 'cleanup': 1,\n",
       " 'missing': 1,\n",
       " 'imputa': 1,\n",
       " 'tion': 1,\n",
       " 'feature': 1,\n",
       " 'selection': 1,\n",
       " 'transformation': 1,\n",
       " 'model': 2,\n",
       " 'discovery': 1,\n",
       " 'criticism': 1,\n",
       " 'explanation': 1,\n",
       " 'allocation': 1,\n",
       " 'computational': 1,\n",
       " 'resources': 1,\n",
       " 'hyperparameter': 1,\n",
       " 'optimization': 1,\n",
       " 'inference': 1,\n",
       " 'monitoring': 1,\n",
       " 'anomaly': 1,\n",
       " 'detection': 1,\n",
       " 'is': 4,\n",
       " 'huge': 1,\n",
       " 'list': 1,\n",
       " 'things': 1,\n",
       " 'd': 1,\n",
       " 'optimally': 1,\n",
       " 'like': 1,\n",
       " 'it': 2,\n",
       " 'there': 1,\n",
       " 'caveat': 1,\n",
       " 'course': 1,\n",
       " 'while': 1,\n",
       " 'full': 1,\n",
       " 'automation': 2,\n",
       " 'can': 1,\n",
       " 'motivate': 1,\n",
       " 'scientific': 1,\n",
       " 'research': 1,\n",
       " 'provide': 1,\n",
       " 'long': 1,\n",
       " 'term': 1,\n",
       " 'engineering': 1,\n",
       " 'goal': 1,\n",
       " 'practice': 2,\n",
       " 'probably': 1,\n",
       " 'semiautomate': 1,\n",
       " 'most': 1,\n",
       " 'these': 2,\n",
       " 'gradually': 1,\n",
       " 'remove': 1,\n",
       " 'human': 1,\n",
       " 'loop': 1,\n",
       " 'as': 1,\n",
       " 'needed': 1,\n",
       " 'along': 1,\n",
       " 'way': 1,\n",
       " 'what': 1,\n",
       " 'going': 1,\n",
       " 'happen': 1,\n",
       " 'if': 1,\n",
       " 'do': 1,\n",
       " 'that': 2,\n",
       " 'are': 1,\n",
       " 'likely': 1,\n",
       " 'develop': 1,\n",
       " 'powerful': 1,\n",
       " 'tools': 1,\n",
       " 'will': 1,\n",
       " 'help': 1,\n",
       " 'make': 1,\n",
       " 'first': 1,\n",
       " 'more': 2,\n",
       " 'systematic': 1,\n",
       " 's': 1,\n",
       " 'ad': 1,\n",
       " 'hoc': 1,\n",
       " 'days': 1,\n",
       " 'also': 1,\n",
       " 'efficient': 1,\n",
       " 'buenos': 1,\n",
       " 'días': 1,\n",
       " 'país': 1,\n",
       " 'subsecretario': 1,\n",
       " 'educación': 1,\n",
       " 'dice': 1,\n",
       " 'comunismo': 1,\n",
       " 'necesario': 1,\n",
       " 'transformar': 1,\n",
       " 'méxico': 1,\n",
       " 'disparan': 1,\n",
       " '312': 1,\n",
       " 'casos': 1,\n",
       " 'dengue': 1,\n",
       " 'compraron': 1,\n",
       " 'insecticidas': 1,\n",
       " 'mireles': 1,\n",
       " 'llama': 1,\n",
       " 'pirujas': 1,\n",
       " 'concubinas': 1,\n",
       " 'presi': 1,\n",
       " 'quiere': 1,\n",
       " 'quitar': 1,\n",
       " 'ine': 1,\n",
       " 'café': 1}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dicc_termns = {}\n",
    "\n",
    "for text in dicc_texts:\n",
    "    for word in dicc_texts[text]:\n",
    "        \n",
    "#        print(\"EVALUAR:\", word, \"EN\", text)\n",
    "        \n",
    "        if(word in dicc_termns):#incrementar palabras al diccionario\n",
    "            dicc_termns[word] = dicc_termns[word] + 1\n",
    "            \n",
    "#            print(word, \"IN\", \"dicc_termns\")\n",
    "            \n",
    "        elif(word not in dicc_termns):#agregar palabras al diccionario        \n",
    "            dicc_termns[word] = 1\n",
    "            \n",
    "#            print(word, \"NOT IN\", \"dicc_termns\")            \n",
    "\n",
    "print(len(dicc_termns))\n",
    "dicc_termns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz Término Documento (binaria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros((len(dicc_texts), len(dicc_termns))) # Pre-allocate matrix\n",
    "#matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i IN text_01\n",
      "se agregó:  1.0 en:  0 0\n",
      "i NOT IN text_02\n",
      "se agregó:  0.0 en:  1 0\n",
      "i NOT IN text_03\n",
      "se agregó:  0.0 en:  2 0\n",
      "have IN text_01\n",
      "se agregó:  1.0 en:  0 1\n",
      "have NOT IN text_02\n",
      "se agregó:  0.0 en:  1 1\n",
      "have NOT IN text_03\n",
      "se agregó:  0.0 en:  2 1\n",
      "been IN text_01\n",
      "se agregó:  1.0 en:  0 2\n",
      "been NOT IN text_02\n",
      "se agregó:  0.0 en:  1 2\n",
      "been NOT IN text_03\n",
      "se agregó:  0.0 en:  2 2\n",
      "very IN text_01\n",
      "se agregó:  1.0 en:  0 3\n",
      "very IN text_02\n",
      "se agregó:  1.0 en:  1 3\n",
      "very NOT IN text_03\n",
      "se agregó:  0.0 en:  2 3\n",
      "passionate IN text_01\n",
      "se agregó:  1.0 en:  0 4\n",
      "passionate NOT IN text_02\n",
      "se agregó:  0.0 en:  1 4\n",
      "passionate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 4\n",
      "about IN text_01\n",
      "se agregó:  1.0 en:  0 5\n",
      "about NOT IN text_02\n",
      "se agregó:  0.0 en:  1 5\n",
      "about NOT IN text_03\n",
      "se agregó:  0.0 en:  2 5\n",
      "automating IN text_01\n",
      "se agregó:  1.0 en:  0 6\n",
      "automating NOT IN text_02\n",
      "se agregó:  0.0 en:  1 6\n",
      "automating NOT IN text_03\n",
      "se agregó:  0.0 en:  2 6\n",
      "machine IN text_01\n",
      "se agregó:  1.0 en:  0 7\n",
      "machine IN text_02\n",
      "se agregó:  1.0 en:  1 7\n",
      "machine NOT IN text_03\n",
      "se agregó:  0.0 en:  2 7\n",
      "learning IN text_01\n",
      "se agregó:  1.0 en:  0 8\n",
      "learning IN text_02\n",
      "se agregó:  1.0 en:  1 8\n",
      "learning NOT IN text_03\n",
      "se agregó:  0.0 en:  2 8\n",
      "myself IN text_01\n",
      "se agregó:  1.0 en:  0 9\n",
      "myself NOT IN text_02\n",
      "se agregó:  0.0 en:  1 9\n",
      "myself NOT IN text_03\n",
      "se agregó:  0.0 en:  2 9\n",
      "ever IN text_01\n",
      "se agregó:  1.0 en:  0 10\n",
      "ever NOT IN text_02\n",
      "se agregó:  0.0 en:  1 10\n",
      "ever NOT IN text_03\n",
      "se agregó:  0.0 en:  2 10\n",
      "since IN text_01\n",
      "se agregó:  1.0 en:  0 11\n",
      "since IN text_02\n",
      "se agregó:  1.0 en:  1 11\n",
      "since NOT IN text_03\n",
      "se agregó:  0.0 en:  2 11\n",
      "our IN text_01\n",
      "se agregó:  1.0 en:  0 12\n",
      "our NOT IN text_02\n",
      "se agregó:  0.0 en:  1 12\n",
      "our NOT IN text_03\n",
      "se agregó:  0.0 en:  2 12\n",
      "automatic IN text_01\n",
      "se agregó:  1.0 en:  0 13\n",
      "automatic NOT IN text_02\n",
      "se agregó:  0.0 en:  1 13\n",
      "automatic NOT IN text_03\n",
      "se agregó:  0.0 en:  2 13\n",
      "statistician IN text_01\n",
      "se agregó:  1.0 en:  0 14\n",
      "statistician NOT IN text_02\n",
      "se agregó:  0.0 en:  1 14\n",
      "statistician NOT IN text_03\n",
      "se agregó:  0.0 en:  2 14\n",
      "project IN text_01\n",
      "se agregó:  1.0 en:  0 15\n",
      "project NOT IN text_02\n",
      "se agregó:  0.0 en:  1 15\n",
      "project NOT IN text_03\n",
      "se agregó:  0.0 en:  2 15\n",
      "started IN text_01\n",
      "se agregó:  1.0 en:  0 16\n",
      "started NOT IN text_02\n",
      "se agregó:  0.0 en:  1 16\n",
      "started NOT IN text_03\n",
      "se agregó:  0.0 en:  2 16\n",
      "back IN text_01\n",
      "se agregó:  1.0 en:  0 17\n",
      "back NOT IN text_02\n",
      "se agregó:  0.0 en:  1 17\n",
      "back NOT IN text_03\n",
      "se agregó:  0.0 en:  2 17\n",
      "in IN text_01\n",
      "se agregó:  1.0 en:  0 18\n",
      "in IN text_02\n",
      "se agregó:  1.0 en:  1 18\n",
      "in NOT IN text_03\n",
      "se agregó:  0.0 en:  2 18\n",
      "2014 IN text_01\n",
      "se agregó:  1.0 en:  0 19\n",
      "2014 NOT IN text_02\n",
      "se agregó:  0.0 en:  1 19\n",
      "2014 NOT IN text_03\n",
      "se agregó:  0.0 en:  2 19\n",
      "want IN text_01\n",
      "se agregó:  1.0 en:  0 20\n",
      "want IN text_02\n",
      "se agregó:  1.0 en:  1 20\n",
      "want NOT IN text_03\n",
      "se agregó:  0.0 en:  2 20\n",
      "us IN text_01\n",
      "se agregó:  1.0 en:  0 21\n",
      "us NOT IN text_02\n",
      "se agregó:  0.0 en:  1 21\n",
      "us NOT IN text_03\n",
      "se agregó:  0.0 en:  2 21\n",
      "to IN text_01\n",
      "se agregó:  1.0 en:  0 22\n",
      "to IN text_02\n",
      "se agregó:  1.0 en:  1 22\n",
      "to NOT IN text_03\n",
      "se agregó:  0.0 en:  2 22\n",
      "be IN text_01\n",
      "se agregó:  1.0 en:  0 23\n",
      "be NOT IN text_02\n",
      "se agregó:  0.0 en:  1 23\n",
      "be NOT IN text_03\n",
      "se agregó:  0.0 en:  2 23\n",
      "really IN text_01\n",
      "se agregó:  1.0 en:  0 24\n",
      "really NOT IN text_02\n",
      "se agregó:  0.0 en:  1 24\n",
      "really NOT IN text_03\n",
      "se agregó:  0.0 en:  2 24\n",
      "ambitious IN text_01\n",
      "se agregó:  1.0 en:  0 25\n",
      "ambitious NOT IN text_02\n",
      "se agregó:  0.0 en:  1 25\n",
      "ambitious NOT IN text_03\n",
      "se agregó:  0.0 en:  2 25\n",
      "this IN text_01\n",
      "se agregó:  1.0 en:  0 26\n",
      "this IN text_02\n",
      "se agregó:  1.0 en:  1 26\n",
      "this NOT IN text_03\n",
      "se agregó:  0.0 en:  2 26\n",
      "endeavor IN text_01\n",
      "se agregó:  1.0 en:  0 27\n",
      "endeavor NOT IN text_02\n",
      "se agregó:  0.0 en:  1 27\n",
      "endeavor NOT IN text_03\n",
      "se agregó:  0.0 en:  2 27\n",
      "we IN text_01\n",
      "se agregó:  1.0 en:  0 28\n",
      "we IN text_02\n",
      "se agregó:  1.0 en:  1 28\n",
      "we NOT IN text_03\n",
      "se agregó:  0.0 en:  2 28\n",
      "should IN text_01\n",
      "se agregó:  1.0 en:  0 29\n",
      "should NOT IN text_02\n",
      "se agregó:  0.0 en:  1 29\n",
      "should NOT IN text_03\n",
      "se agregó:  0.0 en:  2 29\n",
      "try IN text_01\n",
      "se agregó:  1.0 en:  0 30\n",
      "try IN text_02\n",
      "se agregó:  1.0 en:  1 30\n",
      "try NOT IN text_03\n",
      "se agregó:  0.0 en:  2 30\n",
      "automate IN text_01\n",
      "se agregó:  1.0 en:  0 31\n",
      "automate NOT IN text_02\n",
      "se agregó:  0.0 en:  1 31\n",
      "automate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 31\n",
      "all IN text_01\n",
      "se agregó:  1.0 en:  0 32\n",
      "all IN text_02\n",
      "se agregó:  1.0 en:  1 32\n",
      "all NOT IN text_03\n",
      "se agregó:  0.0 en:  2 32\n",
      "aspects IN text_01\n",
      "se agregó:  1.0 en:  0 33\n",
      "aspects NOT IN text_02\n",
      "se agregó:  0.0 en:  1 33\n",
      "aspects NOT IN text_03\n",
      "se agregó:  0.0 en:  2 33\n",
      "of IN text_01\n",
      "se agregó:  1.0 en:  0 34\n",
      "of IN text_02\n",
      "se agregó:  1.0 en:  1 34\n",
      "of NOT IN text_03\n",
      "se agregó:  0.0 en:  2 34\n",
      "the IN text_01\n",
      "se agregó:  1.0 en:  0 35\n",
      "the IN text_02\n",
      "se agregó:  1.0 en:  1 35\n",
      "the NOT IN text_03\n",
      "se agregó:  0.0 en:  2 35\n",
      "entire IN text_01\n",
      "se agregó:  1.0 en:  0 36\n",
      "entire NOT IN text_02\n",
      "se agregó:  0.0 en:  1 36\n",
      "entire NOT IN text_03\n",
      "se agregó:  0.0 en:  2 36\n",
      "and IN text_01\n",
      "se agregó:  1.0 en:  0 37\n",
      "and IN text_02\n",
      "se agregó:  1.0 en:  1 37\n",
      "and NOT IN text_03\n",
      "se agregó:  0.0 en:  2 37\n",
      "data IN text_01\n",
      "se agregó:  1.0 en:  0 38\n",
      "data NOT IN text_02\n",
      "se agregó:  0.0 en:  1 38\n",
      "data NOT IN text_03\n",
      "se agregó:  0.0 en:  2 38\n",
      "analysis IN text_01\n",
      "se agregó:  1.0 en:  0 39\n",
      "analysis NOT IN text_02\n",
      "se agregó:  0.0 en:  1 39\n",
      "analysis NOT IN text_03\n",
      "se agregó:  0.0 en:  2 39\n",
      "pipeline IN text_01\n",
      "se agregó:  1.0 en:  0 40\n",
      "pipeline NOT IN text_02\n",
      "se agregó:  0.0 en:  1 40\n",
      "pipeline NOT IN text_03\n",
      "se agregó:  0.0 en:  2 40\n",
      "includes IN text_01\n",
      "se agregó:  1.0 en:  0 41\n",
      "includes NOT IN text_02\n",
      "se agregó:  0.0 en:  1 41\n",
      "includes NOT IN text_03\n",
      "se agregó:  0.0 en:  2 41\n",
      "collection IN text_01\n",
      "se agregó:  1.0 en:  0 42\n",
      "collection NOT IN text_02\n",
      "se agregó:  0.0 en:  1 42\n",
      "collection NOT IN text_03\n",
      "se agregó:  0.0 en:  2 42\n",
      "experiment IN text_01\n",
      "se agregó:  1.0 en:  0 43\n",
      "experiment NOT IN text_02\n",
      "se agregó:  0.0 en:  1 43\n",
      "experiment NOT IN text_03\n",
      "se agregó:  0.0 en:  2 43\n",
      "design IN text_01\n",
      "se agregó:  1.0 en:  0 44\n",
      "design NOT IN text_02\n",
      "se agregó:  0.0 en:  1 44\n",
      "design NOT IN text_03\n",
      "se agregó:  0.0 en:  2 44\n",
      "cleanup IN text_01\n",
      "se agregó:  1.0 en:  0 45\n",
      "cleanup NOT IN text_02\n",
      "se agregó:  0.0 en:  1 45\n",
      "cleanup NOT IN text_03\n",
      "se agregó:  0.0 en:  2 45\n",
      "missing IN text_01\n",
      "se agregó:  1.0 en:  0 46\n",
      "missing NOT IN text_02\n",
      "se agregó:  0.0 en:  1 46\n",
      "missing NOT IN text_03\n",
      "se agregó:  0.0 en:  2 46\n",
      "imputa IN text_01\n",
      "se agregó:  1.0 en:  0 47\n",
      "imputa NOT IN text_02\n",
      "se agregó:  0.0 en:  1 47\n",
      "imputa NOT IN text_03\n",
      "se agregó:  0.0 en:  2 47\n",
      "tion IN text_01\n",
      "se agregó:  1.0 en:  0 48\n",
      "tion NOT IN text_02\n",
      "se agregó:  0.0 en:  1 48\n",
      "tion NOT IN text_03\n",
      "se agregó:  0.0 en:  2 48\n",
      "feature IN text_01\n",
      "se agregó:  1.0 en:  0 49\n",
      "feature NOT IN text_02\n",
      "se agregó:  0.0 en:  1 49\n",
      "feature NOT IN text_03\n",
      "se agregó:  0.0 en:  2 49\n",
      "selection IN text_01\n",
      "se agregó:  1.0 en:  0 50\n",
      "selection NOT IN text_02\n",
      "se agregó:  0.0 en:  1 50\n",
      "selection NOT IN text_03\n",
      "se agregó:  0.0 en:  2 50\n",
      "transformation IN text_01\n",
      "se agregó:  1.0 en:  0 51\n",
      "transformation NOT IN text_02\n",
      "se agregó:  0.0 en:  1 51\n",
      "transformation NOT IN text_03\n",
      "se agregó:  0.0 en:  2 51\n",
      "model IN text_01\n",
      "se agregó:  1.0 en:  0 52\n",
      "model NOT IN text_02\n",
      "se agregó:  0.0 en:  1 52\n",
      "model NOT IN text_03\n",
      "se agregó:  0.0 en:  2 52\n",
      "discovery IN text_01\n",
      "se agregó:  1.0 en:  0 53\n",
      "discovery NOT IN text_02\n",
      "se agregó:  0.0 en:  1 53\n",
      "discovery NOT IN text_03\n",
      "se agregó:  0.0 en:  2 53\n",
      "criticism IN text_01\n",
      "se agregó:  1.0 en:  0 54\n",
      "criticism NOT IN text_02\n",
      "se agregó:  0.0 en:  1 54\n",
      "criticism NOT IN text_03\n",
      "se agregó:  0.0 en:  2 54\n",
      "explanation IN text_01\n",
      "se agregó:  1.0 en:  0 55\n",
      "explanation NOT IN text_02\n",
      "se agregó:  0.0 en:  1 55\n",
      "explanation NOT IN text_03\n",
      "se agregó:  0.0 en:  2 55\n",
      "allocation IN text_01\n",
      "se agregó:  1.0 en:  0 56\n",
      "allocation NOT IN text_02\n",
      "se agregó:  0.0 en:  1 56\n",
      "allocation NOT IN text_03\n",
      "se agregó:  0.0 en:  2 56\n",
      "computational IN text_01\n",
      "se agregó:  1.0 en:  0 57\n",
      "computational NOT IN text_02\n",
      "se agregó:  0.0 en:  1 57\n",
      "computational NOT IN text_03\n",
      "se agregó:  0.0 en:  2 57\n",
      "resources IN text_01\n",
      "se agregó:  1.0 en:  0 58\n",
      "resources NOT IN text_02\n",
      "se agregó:  0.0 en:  1 58\n",
      "resources NOT IN text_03\n",
      "se agregó:  0.0 en:  2 58\n",
      "hyperparameter IN text_01\n",
      "se agregó:  1.0 en:  0 59\n",
      "hyperparameter NOT IN text_02\n",
      "se agregó:  0.0 en:  1 59\n",
      "hyperparameter NOT IN text_03\n",
      "se agregó:  0.0 en:  2 59\n",
      "optimization IN text_01\n",
      "se agregó:  1.0 en:  0 60\n",
      "optimization NOT IN text_02\n",
      "se agregó:  0.0 en:  1 60\n",
      "optimization NOT IN text_03\n",
      "se agregó:  0.0 en:  2 60\n",
      "inference IN text_01\n",
      "se agregó:  1.0 en:  0 61\n",
      "inference NOT IN text_02\n",
      "se agregó:  0.0 en:  1 61\n",
      "inference NOT IN text_03\n",
      "se agregó:  0.0 en:  2 61\n",
      "monitoring IN text_01\n",
      "se agregó:  1.0 en:  0 62\n",
      "monitoring NOT IN text_02\n",
      "se agregó:  0.0 en:  1 62\n",
      "monitoring NOT IN text_03\n",
      "se agregó:  0.0 en:  2 62\n",
      "anomaly IN text_01\n",
      "se agregó:  1.0 en:  0 63\n",
      "anomaly NOT IN text_02\n",
      "se agregó:  0.0 en:  1 63\n",
      "anomaly NOT IN text_03\n",
      "se agregó:  0.0 en:  2 63\n",
      "detection IN text_01\n",
      "se agregó:  1.0 en:  0 64\n",
      "detection NOT IN text_02\n",
      "se agregó:  0.0 en:  1 64\n",
      "detection NOT IN text_03\n",
      "se agregó:  0.0 en:  2 64\n",
      "is IN text_01\n",
      "se agregó:  1.0 en:  0 65\n",
      "is IN text_02\n",
      "se agregó:  1.0 en:  1 65\n",
      "is NOT IN text_03\n",
      "se agregó:  0.0 en:  2 65\n",
      "huge IN text_01\n",
      "se agregó:  1.0 en:  0 66\n",
      "huge NOT IN text_02\n",
      "se agregó:  0.0 en:  1 66\n",
      "huge NOT IN text_03\n",
      "se agregó:  0.0 en:  2 66\n",
      "list IN text_01\n",
      "se agregó:  1.0 en:  0 67\n",
      "list NOT IN text_02\n",
      "se agregó:  0.0 en:  1 67\n",
      "list NOT IN text_03\n",
      "se agregó:  0.0 en:  2 67\n",
      "things IN text_01\n",
      "se agregó:  1.0 en:  0 68\n",
      "things NOT IN text_02\n",
      "se agregó:  0.0 en:  1 68\n",
      "things NOT IN text_03\n",
      "se agregó:  0.0 en:  2 68\n",
      "d IN text_01\n",
      "se agregó:  1.0 en:  0 69\n",
      "d NOT IN text_02\n",
      "se agregó:  0.0 en:  1 69\n",
      "d NOT IN text_03\n",
      "se agregó:  0.0 en:  2 69\n",
      "optimally IN text_01\n",
      "se agregó:  1.0 en:  0 70\n",
      "optimally NOT IN text_02\n",
      "se agregó:  0.0 en:  1 70\n",
      "optimally NOT IN text_03\n",
      "se agregó:  0.0 en:  2 70\n",
      "like IN text_01\n",
      "se agregó:  1.0 en:  0 71\n",
      "like NOT IN text_02\n",
      "se agregó:  0.0 en:  1 71\n",
      "like NOT IN text_03\n",
      "se agregó:  0.0 en:  2 71\n",
      "it IN text_01\n",
      "se agregó:  1.0 en:  0 72\n",
      "it IN text_02\n",
      "se agregó:  1.0 en:  1 72\n",
      "it NOT IN text_03\n",
      "se agregó:  0.0 en:  2 72\n",
      "there NOT IN text_01\n",
      "se agregó:  0.0 en:  0 73\n",
      "there IN text_02\n",
      "se agregó:  1.0 en:  1 73\n",
      "there NOT IN text_03\n",
      "se agregó:  0.0 en:  2 73\n",
      "caveat NOT IN text_01\n",
      "se agregó:  0.0 en:  0 74\n",
      "caveat IN text_02\n",
      "se agregó:  1.0 en:  1 74\n",
      "caveat NOT IN text_03\n",
      "se agregó:  0.0 en:  2 74\n",
      "course NOT IN text_01\n",
      "se agregó:  0.0 en:  0 75\n",
      "course IN text_02\n",
      "se agregó:  1.0 en:  1 75\n",
      "course NOT IN text_03\n",
      "se agregó:  0.0 en:  2 75\n",
      "while NOT IN text_01\n",
      "se agregó:  0.0 en:  0 76\n",
      "while IN text_02\n",
      "se agregó:  1.0 en:  1 76\n",
      "while NOT IN text_03\n",
      "se agregó:  0.0 en:  2 76\n",
      "full NOT IN text_01\n",
      "se agregó:  0.0 en:  0 77\n",
      "full IN text_02\n",
      "se agregó:  1.0 en:  1 77\n",
      "full NOT IN text_03\n",
      "se agregó:  0.0 en:  2 77\n",
      "automation NOT IN text_01\n",
      "se agregó:  0.0 en:  0 78\n",
      "automation IN text_02\n",
      "se agregó:  1.0 en:  1 78\n",
      "automation NOT IN text_03\n",
      "se agregó:  0.0 en:  2 78\n",
      "can NOT IN text_01\n",
      "se agregó:  0.0 en:  0 79\n",
      "can IN text_02\n",
      "se agregó:  1.0 en:  1 79\n",
      "can NOT IN text_03\n",
      "se agregó:  0.0 en:  2 79\n",
      "motivate NOT IN text_01\n",
      "se agregó:  0.0 en:  0 80\n",
      "motivate IN text_02\n",
      "se agregó:  1.0 en:  1 80\n",
      "motivate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 80\n",
      "scientific NOT IN text_01\n",
      "se agregó:  0.0 en:  0 81\n",
      "scientific IN text_02\n",
      "se agregó:  1.0 en:  1 81\n",
      "scientific NOT IN text_03\n",
      "se agregó:  0.0 en:  2 81\n",
      "research NOT IN text_01\n",
      "se agregó:  0.0 en:  0 82\n",
      "research IN text_02\n",
      "se agregó:  1.0 en:  1 82\n",
      "research NOT IN text_03\n",
      "se agregó:  0.0 en:  2 82\n",
      "provide NOT IN text_01\n",
      "se agregó:  0.0 en:  0 83\n",
      "provide IN text_02\n",
      "se agregó:  1.0 en:  1 83\n",
      "provide NOT IN text_03\n",
      "se agregó:  0.0 en:  2 83\n",
      "long NOT IN text_01\n",
      "se agregó:  0.0 en:  0 84\n",
      "long IN text_02\n",
      "se agregó:  1.0 en:  1 84\n",
      "long NOT IN text_03\n",
      "se agregó:  0.0 en:  2 84\n",
      "term NOT IN text_01\n",
      "se agregó:  0.0 en:  0 85\n",
      "term IN text_02\n",
      "se agregó:  1.0 en:  1 85\n",
      "term NOT IN text_03\n",
      "se agregó:  0.0 en:  2 85\n",
      "engineering NOT IN text_01\n",
      "se agregó:  0.0 en:  0 86\n",
      "engineering IN text_02\n",
      "se agregó:  1.0 en:  1 86\n",
      "engineering NOT IN text_03\n",
      "se agregó:  0.0 en:  2 86\n",
      "goal NOT IN text_01\n",
      "se agregó:  0.0 en:  0 87\n",
      "goal IN text_02\n",
      "se agregó:  1.0 en:  1 87\n",
      "goal NOT IN text_03\n",
      "se agregó:  0.0 en:  2 87\n",
      "practice NOT IN text_01\n",
      "se agregó:  0.0 en:  0 88\n",
      "practice IN text_02\n",
      "se agregó:  1.0 en:  1 88\n",
      "practice NOT IN text_03\n",
      "se agregó:  0.0 en:  2 88\n",
      "probably NOT IN text_01\n",
      "se agregó:  0.0 en:  0 89\n",
      "probably IN text_02\n",
      "se agregó:  1.0 en:  1 89\n",
      "probably NOT IN text_03\n",
      "se agregó:  0.0 en:  2 89\n",
      "semiautomate NOT IN text_01\n",
      "se agregó:  0.0 en:  0 90\n",
      "semiautomate IN text_02\n",
      "se agregó:  1.0 en:  1 90\n",
      "semiautomate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 90\n",
      "most NOT IN text_01\n",
      "se agregó:  0.0 en:  0 91\n",
      "most IN text_02\n",
      "se agregó:  1.0 en:  1 91\n",
      "most NOT IN text_03\n",
      "se agregó:  0.0 en:  2 91\n",
      "these NOT IN text_01\n",
      "se agregó:  0.0 en:  0 92\n",
      "these IN text_02\n",
      "se agregó:  1.0 en:  1 92\n",
      "these NOT IN text_03\n",
      "se agregó:  0.0 en:  2 92\n",
      "gradually NOT IN text_01\n",
      "se agregó:  0.0 en:  0 93\n",
      "gradually IN text_02\n",
      "se agregó:  1.0 en:  1 93\n",
      "gradually NOT IN text_03\n",
      "se agregó:  0.0 en:  2 93\n",
      "remove NOT IN text_01\n",
      "se agregó:  0.0 en:  0 94\n",
      "remove IN text_02\n",
      "se agregó:  1.0 en:  1 94\n",
      "remove NOT IN text_03\n",
      "se agregó:  0.0 en:  2 94\n",
      "human NOT IN text_01\n",
      "se agregó:  0.0 en:  0 95\n",
      "human IN text_02\n",
      "se agregó:  1.0 en:  1 95\n",
      "human NOT IN text_03\n",
      "se agregó:  0.0 en:  2 95\n",
      "loop NOT IN text_01\n",
      "se agregó:  0.0 en:  0 96\n",
      "loop IN text_02\n",
      "se agregó:  1.0 en:  1 96\n",
      "loop NOT IN text_03\n",
      "se agregó:  0.0 en:  2 96\n",
      "as NOT IN text_01\n",
      "se agregó:  0.0 en:  0 97\n",
      "as IN text_02\n",
      "se agregó:  1.0 en:  1 97\n",
      "as NOT IN text_03\n",
      "se agregó:  0.0 en:  2 97\n",
      "needed NOT IN text_01\n",
      "se agregó:  0.0 en:  0 98\n",
      "needed IN text_02\n",
      "se agregó:  1.0 en:  1 98\n",
      "needed NOT IN text_03\n",
      "se agregó:  0.0 en:  2 98\n",
      "along NOT IN text_01\n",
      "se agregó:  0.0 en:  0 99\n",
      "along IN text_02\n",
      "se agregó:  1.0 en:  1 99\n",
      "along NOT IN text_03\n",
      "se agregó:  0.0 en:  2 99\n",
      "way NOT IN text_01\n",
      "se agregó:  0.0 en:  0 100\n",
      "way IN text_02\n",
      "se agregó:  1.0 en:  1 100\n",
      "way NOT IN text_03\n",
      "se agregó:  0.0 en:  2 100\n",
      "what NOT IN text_01\n",
      "se agregó:  0.0 en:  0 101\n",
      "what IN text_02\n",
      "se agregó:  1.0 en:  1 101\n",
      "what NOT IN text_03\n",
      "se agregó:  0.0 en:  2 101\n",
      "going NOT IN text_01\n",
      "se agregó:  0.0 en:  0 102\n",
      "going IN text_02\n",
      "se agregó:  1.0 en:  1 102\n",
      "going NOT IN text_03\n",
      "se agregó:  0.0 en:  2 102\n",
      "happen NOT IN text_01\n",
      "se agregó:  0.0 en:  0 103\n",
      "happen IN text_02\n",
      "se agregó:  1.0 en:  1 103\n",
      "happen NOT IN text_03\n",
      "se agregó:  0.0 en:  2 103\n",
      "if NOT IN text_01\n",
      "se agregó:  0.0 en:  0 104\n",
      "if IN text_02\n",
      "se agregó:  1.0 en:  1 104\n",
      "if NOT IN text_03\n",
      "se agregó:  0.0 en:  2 104\n",
      "do NOT IN text_01\n",
      "se agregó:  0.0 en:  0 105\n",
      "do IN text_02\n",
      "se agregó:  1.0 en:  1 105\n",
      "do NOT IN text_03\n",
      "se agregó:  0.0 en:  2 105\n",
      "that NOT IN text_01\n",
      "se agregó:  0.0 en:  0 106\n",
      "that IN text_02\n",
      "se agregó:  1.0 en:  1 106\n",
      "that NOT IN text_03\n",
      "se agregó:  0.0 en:  2 106\n",
      "are NOT IN text_01\n",
      "se agregó:  0.0 en:  0 107\n",
      "are IN text_02\n",
      "se agregó:  1.0 en:  1 107\n",
      "are NOT IN text_03\n",
      "se agregó:  0.0 en:  2 107\n",
      "likely NOT IN text_01\n",
      "se agregó:  0.0 en:  0 108\n",
      "likely IN text_02\n",
      "se agregó:  1.0 en:  1 108\n",
      "likely NOT IN text_03\n",
      "se agregó:  0.0 en:  2 108\n",
      "develop NOT IN text_01\n",
      "se agregó:  0.0 en:  0 109\n",
      "develop IN text_02\n",
      "se agregó:  1.0 en:  1 109\n",
      "develop NOT IN text_03\n",
      "se agregó:  0.0 en:  2 109\n",
      "powerful NOT IN text_01\n",
      "se agregó:  0.0 en:  0 110\n",
      "powerful IN text_02\n",
      "se agregó:  1.0 en:  1 110\n",
      "powerful NOT IN text_03\n",
      "se agregó:  0.0 en:  2 110\n",
      "tools NOT IN text_01\n",
      "se agregó:  0.0 en:  0 111\n",
      "tools IN text_02\n",
      "se agregó:  1.0 en:  1 111\n",
      "tools NOT IN text_03\n",
      "se agregó:  0.0 en:  2 111\n",
      "will NOT IN text_01\n",
      "se agregó:  0.0 en:  0 112\n",
      "will IN text_02\n",
      "se agregó:  1.0 en:  1 112\n",
      "will NOT IN text_03\n",
      "se agregó:  0.0 en:  2 112\n",
      "help NOT IN text_01\n",
      "se agregó:  0.0 en:  0 113\n",
      "help IN text_02\n",
      "se agregó:  1.0 en:  1 113\n",
      "help NOT IN text_03\n",
      "se agregó:  0.0 en:  2 113\n",
      "make NOT IN text_01\n",
      "se agregó:  0.0 en:  0 114\n",
      "make IN text_02\n",
      "se agregó:  1.0 en:  1 114\n",
      "make NOT IN text_03\n",
      "se agregó:  0.0 en:  2 114\n",
      "first NOT IN text_01\n",
      "se agregó:  0.0 en:  0 115\n",
      "first IN text_02\n",
      "se agregó:  1.0 en:  1 115\n",
      "first NOT IN text_03\n",
      "se agregó:  0.0 en:  2 115\n",
      "more NOT IN text_01\n",
      "se agregó:  0.0 en:  0 116\n",
      "more IN text_02\n",
      "se agregó:  1.0 en:  1 116\n",
      "more NOT IN text_03\n",
      "se agregó:  0.0 en:  2 116\n",
      "systematic NOT IN text_01\n",
      "se agregó:  0.0 en:  0 117\n",
      "systematic IN text_02\n",
      "se agregó:  1.0 en:  1 117\n",
      "systematic NOT IN text_03\n",
      "se agregó:  0.0 en:  2 117\n",
      "s NOT IN text_01\n",
      "se agregó:  0.0 en:  0 118\n",
      "s IN text_02\n",
      "se agregó:  1.0 en:  1 118\n",
      "s NOT IN text_03\n",
      "se agregó:  0.0 en:  2 118\n",
      "ad NOT IN text_01\n",
      "se agregó:  0.0 en:  0 119\n",
      "ad IN text_02\n",
      "se agregó:  1.0 en:  1 119\n",
      "ad NOT IN text_03\n",
      "se agregó:  0.0 en:  2 119\n",
      "hoc NOT IN text_01\n",
      "se agregó:  0.0 en:  0 120\n",
      "hoc IN text_02\n",
      "se agregó:  1.0 en:  1 120\n",
      "hoc NOT IN text_03\n",
      "se agregó:  0.0 en:  2 120\n",
      "days NOT IN text_01\n",
      "se agregó:  0.0 en:  0 121\n",
      "days IN text_02\n",
      "se agregó:  1.0 en:  1 121\n",
      "days NOT IN text_03\n",
      "se agregó:  0.0 en:  2 121\n",
      "also NOT IN text_01\n",
      "se agregó:  0.0 en:  0 122\n",
      "also IN text_02\n",
      "se agregó:  1.0 en:  1 122\n",
      "also NOT IN text_03\n",
      "se agregó:  0.0 en:  2 122\n",
      "efficient NOT IN text_01\n",
      "se agregó:  0.0 en:  0 123\n",
      "efficient IN text_02\n",
      "se agregó:  1.0 en:  1 123\n",
      "efficient NOT IN text_03\n",
      "se agregó:  0.0 en:  2 123\n",
      "buenos NOT IN text_01\n",
      "se agregó:  0.0 en:  0 124\n",
      "buenos NOT IN text_02\n",
      "se agregó:  0.0 en:  1 124\n",
      "buenos IN text_03\n",
      "se agregó:  1.0 en:  2 124\n",
      "días NOT IN text_01\n",
      "se agregó:  0.0 en:  0 125\n",
      "días NOT IN text_02\n",
      "se agregó:  0.0 en:  1 125\n",
      "días IN text_03\n",
      "se agregó:  1.0 en:  2 125\n",
      "país NOT IN text_01\n",
      "se agregó:  0.0 en:  0 126\n",
      "país NOT IN text_02\n",
      "se agregó:  0.0 en:  1 126\n",
      "país IN text_03\n",
      "se agregó:  1.0 en:  2 126\n",
      "subsecretario NOT IN text_01\n",
      "se agregó:  0.0 en:  0 127\n",
      "subsecretario NOT IN text_02\n",
      "se agregó:  0.0 en:  1 127\n",
      "subsecretario IN text_03\n",
      "se agregó:  1.0 en:  2 127\n",
      "educación NOT IN text_01\n",
      "se agregó:  0.0 en:  0 128\n",
      "educación NOT IN text_02\n",
      "se agregó:  0.0 en:  1 128\n",
      "educación IN text_03\n",
      "se agregó:  1.0 en:  2 128\n",
      "dice NOT IN text_01\n",
      "se agregó:  0.0 en:  0 129\n",
      "dice NOT IN text_02\n",
      "se agregó:  0.0 en:  1 129\n",
      "dice IN text_03\n",
      "se agregó:  1.0 en:  2 129\n",
      "comunismo NOT IN text_01\n",
      "se agregó:  0.0 en:  0 130\n",
      "comunismo NOT IN text_02\n",
      "se agregó:  0.0 en:  1 130\n",
      "comunismo IN text_03\n",
      "se agregó:  1.0 en:  2 130\n",
      "necesario NOT IN text_01\n",
      "se agregó:  0.0 en:  0 131\n",
      "necesario NOT IN text_02\n",
      "se agregó:  0.0 en:  1 131\n",
      "necesario IN text_03\n",
      "se agregó:  1.0 en:  2 131\n",
      "transformar NOT IN text_01\n",
      "se agregó:  0.0 en:  0 132\n",
      "transformar NOT IN text_02\n",
      "se agregó:  0.0 en:  1 132\n",
      "transformar IN text_03\n",
      "se agregó:  1.0 en:  2 132\n",
      "méxico NOT IN text_01\n",
      "se agregó:  0.0 en:  0 133\n",
      "méxico NOT IN text_02\n",
      "se agregó:  0.0 en:  1 133\n",
      "méxico IN text_03\n",
      "se agregó:  1.0 en:  2 133\n",
      "disparan NOT IN text_01\n",
      "se agregó:  0.0 en:  0 134\n",
      "disparan NOT IN text_02\n",
      "se agregó:  0.0 en:  1 134\n",
      "disparan IN text_03\n",
      "se agregó:  1.0 en:  2 134\n",
      "312 NOT IN text_01\n",
      "se agregó:  0.0 en:  0 135\n",
      "312 NOT IN text_02\n",
      "se agregó:  0.0 en:  1 135\n",
      "312 IN text_03\n",
      "se agregó:  1.0 en:  2 135\n",
      "casos NOT IN text_01\n",
      "se agregó:  0.0 en:  0 136\n",
      "casos NOT IN text_02\n",
      "se agregó:  0.0 en:  1 136\n",
      "casos IN text_03\n",
      "se agregó:  1.0 en:  2 136\n",
      "dengue NOT IN text_01\n",
      "se agregó:  0.0 en:  0 137\n",
      "dengue NOT IN text_02\n",
      "se agregó:  0.0 en:  1 137\n",
      "dengue IN text_03\n",
      "se agregó:  1.0 en:  2 137\n",
      "compraron NOT IN text_01\n",
      "se agregó:  0.0 en:  0 138\n",
      "compraron NOT IN text_02\n",
      "se agregó:  0.0 en:  1 138\n",
      "compraron IN text_03\n",
      "se agregó:  1.0 en:  2 138\n",
      "insecticidas NOT IN text_01\n",
      "se agregó:  0.0 en:  0 139\n",
      "insecticidas NOT IN text_02\n",
      "se agregó:  0.0 en:  1 139\n",
      "insecticidas IN text_03\n",
      "se agregó:  1.0 en:  2 139\n",
      "mireles NOT IN text_01\n",
      "se agregó:  0.0 en:  0 140\n",
      "mireles NOT IN text_02\n",
      "se agregó:  0.0 en:  1 140\n",
      "mireles IN text_03\n",
      "se agregó:  1.0 en:  2 140\n",
      "llama NOT IN text_01\n",
      "se agregó:  0.0 en:  0 141\n",
      "llama NOT IN text_02\n",
      "se agregó:  0.0 en:  1 141\n",
      "llama IN text_03\n",
      "se agregó:  1.0 en:  2 141\n",
      "pirujas NOT IN text_01\n",
      "se agregó:  0.0 en:  0 142\n",
      "pirujas NOT IN text_02\n",
      "se agregó:  0.0 en:  1 142\n",
      "pirujas IN text_03\n",
      "se agregó:  1.0 en:  2 142\n",
      "concubinas NOT IN text_01\n",
      "se agregó:  0.0 en:  0 143\n",
      "concubinas NOT IN text_02\n",
      "se agregó:  0.0 en:  1 143\n",
      "concubinas IN text_03\n",
      "se agregó:  1.0 en:  2 143\n",
      "presi NOT IN text_01\n",
      "se agregó:  0.0 en:  0 144\n",
      "presi NOT IN text_02\n",
      "se agregó:  0.0 en:  1 144\n",
      "presi IN text_03\n",
      "se agregó:  1.0 en:  2 144\n",
      "quiere NOT IN text_01\n",
      "se agregó:  0.0 en:  0 145\n",
      "quiere NOT IN text_02\n",
      "se agregó:  0.0 en:  1 145\n",
      "quiere IN text_03\n",
      "se agregó:  1.0 en:  2 145\n",
      "quitar NOT IN text_01\n",
      "se agregó:  0.0 en:  0 146\n",
      "quitar NOT IN text_02\n",
      "se agregó:  0.0 en:  1 146\n",
      "quitar IN text_03\n",
      "se agregó:  1.0 en:  2 146\n",
      "ine NOT IN text_01\n",
      "se agregó:  0.0 en:  0 147\n",
      "ine NOT IN text_02\n",
      "se agregó:  0.0 en:  1 147\n",
      "ine IN text_03\n",
      "se agregó:  1.0 en:  2 147\n",
      "café NOT IN text_01\n",
      "se agregó:  0.0 en:  0 148\n",
      "café NOT IN text_02\n",
      "se agregó:  0.0 en:  1 148\n",
      "café IN text_03\n",
      "se agregó:  1.0 en:  2 148\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for word_termns in dicc_termns: #dicc_termns todos los términos\n",
    "#    print()\n",
    "    for word_texts in dicc_texts: #dicc_texts todos los textos\n",
    "#        print(\"EVALUAR:\", word_termns, \"EN: \", word_texts)\n",
    "        if(word_termns in dicc_texts[word_texts]): #si está\n",
    "            print(word_termns, \"IN\", word_texts)\n",
    "            \n",
    "            matrix[j, i] = 1\n",
    "            \n",
    "        elif(word_termns not in dicc_texts[word_texts]): # si no está\n",
    "            print(word_termns, \"NOT IN\", word_texts)\n",
    "            \n",
    "            matrix[j, i] = 0\n",
    "            \n",
    "            \n",
    "        print(\"se agregó: \", matrix[j,i], \"en: \", j, i)\n",
    "            \n",
    "        j = j + 1\n",
    "        \n",
    "    j = 0\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
       "        1., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 149)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22878163071641286"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_cos_t01_t02 = dot(matrix[0],matrix[1])/(norm(matrix[0])*norm(matrix[1]))\n",
    "bin_cos_t01_t02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_cos_t02_t03 = dot(matrix[1],matrix[2])/(norm(matrix[1])*norm(matrix[2]))\n",
    "bin_cos_t02_t03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bin_cos_t01_t03 = dot(matrix[0],matrix[2])/(norm(matrix[0])*norm(matrix[2]))\n",
    "bin_cos_t01_t03"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matriz Término Documento (frecuencia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.zeros((len(dicc_texts), len(dicc_termns))) # Pre-allocate matrix\n",
    "#matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i IN text_01\n",
      "se agregó:  2.0 en:  0 0\n",
      "i NOT IN text_02\n",
      "se agregó:  0.0 en:  1 0\n",
      "i NOT IN text_03\n",
      "se agregó:  0.0 en:  2 0\n",
      "have IN text_01\n",
      "se agregó:  1.0 en:  0 1\n",
      "have NOT IN text_02\n",
      "se agregó:  0.0 en:  1 1\n",
      "have NOT IN text_03\n",
      "se agregó:  0.0 en:  2 1\n",
      "been IN text_01\n",
      "se agregó:  1.0 en:  0 2\n",
      "been NOT IN text_02\n",
      "se agregó:  0.0 en:  1 2\n",
      "been NOT IN text_03\n",
      "se agregó:  0.0 en:  2 2\n",
      "very IN text_01\n",
      "se agregó:  2.0 en:  0 3\n",
      "very IN text_02\n",
      "se agregó:  2.0 en:  1 3\n",
      "very NOT IN text_03\n",
      "se agregó:  0.0 en:  2 3\n",
      "passionate IN text_01\n",
      "se agregó:  1.0 en:  0 4\n",
      "passionate NOT IN text_02\n",
      "se agregó:  0.0 en:  1 4\n",
      "passionate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 4\n",
      "about IN text_01\n",
      "se agregó:  1.0 en:  0 5\n",
      "about NOT IN text_02\n",
      "se agregó:  0.0 en:  1 5\n",
      "about NOT IN text_03\n",
      "se agregó:  0.0 en:  2 5\n",
      "automating IN text_01\n",
      "se agregó:  9.0 en:  0 6\n",
      "automating NOT IN text_02\n",
      "se agregó:  0.0 en:  1 6\n",
      "automating NOT IN text_03\n",
      "se agregó:  0.0 en:  2 6\n",
      "machine IN text_01\n",
      "se agregó:  3.0 en:  0 7\n",
      "machine IN text_02\n",
      "se agregó:  3.0 en:  1 7\n",
      "machine NOT IN text_03\n",
      "se agregó:  0.0 en:  2 7\n",
      "learning IN text_01\n",
      "se agregó:  3.0 en:  0 8\n",
      "learning IN text_02\n",
      "se agregó:  3.0 en:  1 8\n",
      "learning NOT IN text_03\n",
      "se agregó:  0.0 en:  2 8\n",
      "myself IN text_01\n",
      "se agregó:  1.0 en:  0 9\n",
      "myself NOT IN text_02\n",
      "se agregó:  0.0 en:  1 9\n",
      "myself NOT IN text_03\n",
      "se agregó:  0.0 en:  2 9\n",
      "ever IN text_01\n",
      "se agregó:  1.0 en:  0 10\n",
      "ever NOT IN text_02\n",
      "se agregó:  0.0 en:  1 10\n",
      "ever NOT IN text_03\n",
      "se agregó:  0.0 en:  2 10\n",
      "since IN text_01\n",
      "se agregó:  2.0 en:  0 11\n",
      "since IN text_02\n",
      "se agregó:  2.0 en:  1 11\n",
      "since NOT IN text_03\n",
      "se agregó:  0.0 en:  2 11\n",
      "our IN text_01\n",
      "se agregó:  1.0 en:  0 12\n",
      "our NOT IN text_02\n",
      "se agregó:  0.0 en:  1 12\n",
      "our NOT IN text_03\n",
      "se agregó:  0.0 en:  2 12\n",
      "automatic IN text_01\n",
      "se agregó:  1.0 en:  0 13\n",
      "automatic NOT IN text_02\n",
      "se agregó:  0.0 en:  1 13\n",
      "automatic NOT IN text_03\n",
      "se agregó:  0.0 en:  2 13\n",
      "statistician IN text_01\n",
      "se agregó:  1.0 en:  0 14\n",
      "statistician NOT IN text_02\n",
      "se agregó:  0.0 en:  1 14\n",
      "statistician NOT IN text_03\n",
      "se agregó:  0.0 en:  2 14\n",
      "project IN text_01\n",
      "se agregó:  1.0 en:  0 15\n",
      "project NOT IN text_02\n",
      "se agregó:  0.0 en:  1 15\n",
      "project NOT IN text_03\n",
      "se agregó:  0.0 en:  2 15\n",
      "started IN text_01\n",
      "se agregó:  1.0 en:  0 16\n",
      "started NOT IN text_02\n",
      "se agregó:  0.0 en:  1 16\n",
      "started NOT IN text_03\n",
      "se agregó:  0.0 en:  2 16\n",
      "back IN text_01\n",
      "se agregó:  1.0 en:  0 17\n",
      "back NOT IN text_02\n",
      "se agregó:  0.0 en:  1 17\n",
      "back NOT IN text_03\n",
      "se agregó:  0.0 en:  2 17\n",
      "in IN text_01\n",
      "se agregó:  4.0 en:  0 18\n",
      "in IN text_02\n",
      "se agregó:  4.0 en:  1 18\n",
      "in NOT IN text_03\n",
      "se agregó:  0.0 en:  2 18\n",
      "2014 IN text_01\n",
      "se agregó:  1.0 en:  0 19\n",
      "2014 NOT IN text_02\n",
      "se agregó:  0.0 en:  1 19\n",
      "2014 NOT IN text_03\n",
      "se agregó:  0.0 en:  2 19\n",
      "want IN text_01\n",
      "se agregó:  2.0 en:  0 20\n",
      "want IN text_02\n",
      "se agregó:  2.0 en:  1 20\n",
      "want NOT IN text_03\n",
      "se agregó:  0.0 en:  2 20\n",
      "us IN text_01\n",
      "se agregó:  1.0 en:  0 21\n",
      "us NOT IN text_02\n",
      "se agregó:  0.0 en:  1 21\n",
      "us NOT IN text_03\n",
      "se agregó:  0.0 en:  2 21\n",
      "to IN text_01\n",
      "se agregó:  7.0 en:  0 22\n",
      "to IN text_02\n",
      "se agregó:  7.0 en:  1 22\n",
      "to NOT IN text_03\n",
      "se agregó:  0.0 en:  2 22\n",
      "be IN text_01\n",
      "se agregó:  1.0 en:  0 23\n",
      "be NOT IN text_02\n",
      "se agregó:  0.0 en:  1 23\n",
      "be NOT IN text_03\n",
      "se agregó:  0.0 en:  2 23\n",
      "really IN text_01\n",
      "se agregó:  1.0 en:  0 24\n",
      "really NOT IN text_02\n",
      "se agregó:  0.0 en:  1 24\n",
      "really NOT IN text_03\n",
      "se agregó:  0.0 en:  2 24\n",
      "ambitious IN text_01\n",
      "se agregó:  1.0 en:  0 25\n",
      "ambitious NOT IN text_02\n",
      "se agregó:  0.0 en:  1 25\n",
      "ambitious NOT IN text_03\n",
      "se agregó:  0.0 en:  2 25\n",
      "this IN text_01\n",
      "se agregó:  4.0 en:  0 26\n",
      "this IN text_02\n",
      "se agregó:  4.0 en:  1 26\n",
      "this NOT IN text_03\n",
      "se agregó:  0.0 en:  2 26\n",
      "endeavor IN text_01\n",
      "se agregó:  1.0 en:  0 27\n",
      "endeavor NOT IN text_02\n",
      "se agregó:  0.0 en:  1 27\n",
      "endeavor NOT IN text_03\n",
      "se agregó:  0.0 en:  2 27\n",
      "we IN text_01\n",
      "se agregó:  5.0 en:  0 28\n",
      "we IN text_02\n",
      "se agregó:  5.0 en:  1 28\n",
      "we NOT IN text_03\n",
      "se agregó:  0.0 en:  2 28\n",
      "should IN text_01\n",
      "se agregó:  1.0 en:  0 29\n",
      "should NOT IN text_02\n",
      "se agregó:  0.0 en:  1 29\n",
      "should NOT IN text_03\n",
      "se agregó:  0.0 en:  2 29\n",
      "try IN text_01\n",
      "se agregó:  2.0 en:  0 30\n",
      "try IN text_02\n",
      "se agregó:  2.0 en:  1 30\n",
      "try NOT IN text_03\n",
      "se agregó:  0.0 en:  2 30\n",
      "automate IN text_01\n",
      "se agregó:  2.0 en:  0 31\n",
      "automate NOT IN text_02\n",
      "se agregó:  0.0 en:  1 31\n",
      "automate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 31\n",
      "all IN text_01\n",
      "se agregó:  4.0 en:  0 32\n",
      "all IN text_02\n",
      "se agregó:  4.0 en:  1 32\n",
      "all NOT IN text_03\n",
      "se agregó:  0.0 en:  2 32\n",
      "aspects IN text_01\n",
      "se agregó:  1.0 en:  0 33\n",
      "aspects NOT IN text_02\n",
      "se agregó:  0.0 en:  1 33\n",
      "aspects NOT IN text_03\n",
      "se agregó:  0.0 en:  2 33\n",
      "of IN text_01\n",
      "se agregó:  8.0 en:  0 34\n",
      "of IN text_02\n",
      "se agregó:  8.0 en:  1 34\n",
      "of NOT IN text_03\n",
      "se agregó:  0.0 en:  2 34\n",
      "the IN text_01\n",
      "se agregó:  6.0 en:  0 35\n",
      "the IN text_02\n",
      "se agregó:  6.0 en:  1 35\n",
      "the NOT IN text_03\n",
      "se agregó:  0.0 en:  2 35\n",
      "entire IN text_01\n",
      "se agregó:  1.0 en:  0 36\n",
      "entire NOT IN text_02\n",
      "se agregó:  0.0 en:  1 36\n",
      "entire NOT IN text_03\n",
      "se agregó:  0.0 en:  2 36\n",
      "and IN text_01\n",
      "se agregó:  11.0 en:  0 37\n",
      "and IN text_02\n",
      "se agregó:  11.0 en:  1 37\n",
      "and NOT IN text_03\n",
      "se agregó:  0.0 en:  2 37\n",
      "data IN text_01\n",
      "se agregó:  4.0 en:  0 38\n",
      "data NOT IN text_02\n",
      "se agregó:  0.0 en:  1 38\n",
      "data NOT IN text_03\n",
      "se agregó:  0.0 en:  2 38\n",
      "analysis IN text_01\n",
      "se agregó:  1.0 en:  0 39\n",
      "analysis NOT IN text_02\n",
      "se agregó:  0.0 en:  1 39\n",
      "analysis NOT IN text_03\n",
      "se agregó:  0.0 en:  2 39\n",
      "pipeline IN text_01\n",
      "se agregó:  1.0 en:  0 40\n",
      "pipeline NOT IN text_02\n",
      "se agregó:  0.0 en:  1 40\n",
      "pipeline NOT IN text_03\n",
      "se agregó:  0.0 en:  2 40\n",
      "includes IN text_01\n",
      "se agregó:  1.0 en:  0 41\n",
      "includes NOT IN text_02\n",
      "se agregó:  0.0 en:  1 41\n",
      "includes NOT IN text_03\n",
      "se agregó:  0.0 en:  2 41\n",
      "collection IN text_01\n",
      "se agregó:  1.0 en:  0 42\n",
      "collection NOT IN text_02\n",
      "se agregó:  0.0 en:  1 42\n",
      "collection NOT IN text_03\n",
      "se agregó:  0.0 en:  2 42\n",
      "experiment IN text_01\n",
      "se agregó:  1.0 en:  0 43\n",
      "experiment NOT IN text_02\n",
      "se agregó:  0.0 en:  1 43\n",
      "experiment NOT IN text_03\n",
      "se agregó:  0.0 en:  2 43\n",
      "design IN text_01\n",
      "se agregó:  1.0 en:  0 44\n",
      "design NOT IN text_02\n",
      "se agregó:  0.0 en:  1 44\n",
      "design NOT IN text_03\n",
      "se agregó:  0.0 en:  2 44\n",
      "cleanup IN text_01\n",
      "se agregó:  1.0 en:  0 45\n",
      "cleanup NOT IN text_02\n",
      "se agregó:  0.0 en:  1 45\n",
      "cleanup NOT IN text_03\n",
      "se agregó:  0.0 en:  2 45\n",
      "missing IN text_01\n",
      "se agregó:  1.0 en:  0 46\n",
      "missing NOT IN text_02\n",
      "se agregó:  0.0 en:  1 46\n",
      "missing NOT IN text_03\n",
      "se agregó:  0.0 en:  2 46\n",
      "imputa IN text_01\n",
      "se agregó:  1.0 en:  0 47\n",
      "imputa NOT IN text_02\n",
      "se agregó:  0.0 en:  1 47\n",
      "imputa NOT IN text_03\n",
      "se agregó:  0.0 en:  2 47\n",
      "tion IN text_01\n",
      "se agregó:  1.0 en:  0 48\n",
      "tion NOT IN text_02\n",
      "se agregó:  0.0 en:  1 48\n",
      "tion NOT IN text_03\n",
      "se agregó:  0.0 en:  2 48\n",
      "feature IN text_01\n",
      "se agregó:  1.0 en:  0 49\n",
      "feature NOT IN text_02\n",
      "se agregó:  0.0 en:  1 49\n",
      "feature NOT IN text_03\n",
      "se agregó:  0.0 en:  2 49\n",
      "selection IN text_01\n",
      "se agregó:  1.0 en:  0 50\n",
      "selection NOT IN text_02\n",
      "se agregó:  0.0 en:  1 50\n",
      "selection NOT IN text_03\n",
      "se agregó:  0.0 en:  2 50\n",
      "transformation IN text_01\n",
      "se agregó:  1.0 en:  0 51\n",
      "transformation NOT IN text_02\n",
      "se agregó:  0.0 en:  1 51\n",
      "transformation NOT IN text_03\n",
      "se agregó:  0.0 en:  2 51\n",
      "model IN text_01\n",
      "se agregó:  2.0 en:  0 52\n",
      "model NOT IN text_02\n",
      "se agregó:  0.0 en:  1 52\n",
      "model NOT IN text_03\n",
      "se agregó:  0.0 en:  2 52\n",
      "discovery IN text_01\n",
      "se agregó:  1.0 en:  0 53\n",
      "discovery NOT IN text_02\n",
      "se agregó:  0.0 en:  1 53\n",
      "discovery NOT IN text_03\n",
      "se agregó:  0.0 en:  2 53\n",
      "criticism IN text_01\n",
      "se agregó:  1.0 en:  0 54\n",
      "criticism NOT IN text_02\n",
      "se agregó:  0.0 en:  1 54\n",
      "criticism NOT IN text_03\n",
      "se agregó:  0.0 en:  2 54\n",
      "explanation IN text_01\n",
      "se agregó:  1.0 en:  0 55\n",
      "explanation NOT IN text_02\n",
      "se agregó:  0.0 en:  1 55\n",
      "explanation NOT IN text_03\n",
      "se agregó:  0.0 en:  2 55\n",
      "allocation IN text_01\n",
      "se agregó:  1.0 en:  0 56\n",
      "allocation NOT IN text_02\n",
      "se agregó:  0.0 en:  1 56\n",
      "allocation NOT IN text_03\n",
      "se agregó:  0.0 en:  2 56\n",
      "computational IN text_01\n",
      "se agregó:  1.0 en:  0 57\n",
      "computational NOT IN text_02\n",
      "se agregó:  0.0 en:  1 57\n",
      "computational NOT IN text_03\n",
      "se agregó:  0.0 en:  2 57\n",
      "resources IN text_01\n",
      "se agregó:  1.0 en:  0 58\n",
      "resources NOT IN text_02\n",
      "se agregó:  0.0 en:  1 58\n",
      "resources NOT IN text_03\n",
      "se agregó:  0.0 en:  2 58\n",
      "hyperparameter IN text_01\n",
      "se agregó:  1.0 en:  0 59\n",
      "hyperparameter NOT IN text_02\n",
      "se agregó:  0.0 en:  1 59\n",
      "hyperparameter NOT IN text_03\n",
      "se agregó:  0.0 en:  2 59\n",
      "optimization IN text_01\n",
      "se agregó:  1.0 en:  0 60\n",
      "optimization NOT IN text_02\n",
      "se agregó:  0.0 en:  1 60\n",
      "optimization NOT IN text_03\n",
      "se agregó:  0.0 en:  2 60\n",
      "inference IN text_01\n",
      "se agregó:  1.0 en:  0 61\n",
      "inference NOT IN text_02\n",
      "se agregó:  0.0 en:  1 61\n",
      "inference NOT IN text_03\n",
      "se agregó:  0.0 en:  2 61\n",
      "monitoring IN text_01\n",
      "se agregó:  1.0 en:  0 62\n",
      "monitoring NOT IN text_02\n",
      "se agregó:  0.0 en:  1 62\n",
      "monitoring NOT IN text_03\n",
      "se agregó:  0.0 en:  2 62\n",
      "anomaly IN text_01\n",
      "se agregó:  1.0 en:  0 63\n",
      "anomaly NOT IN text_02\n",
      "se agregó:  0.0 en:  1 63\n",
      "anomaly NOT IN text_03\n",
      "se agregó:  0.0 en:  2 63\n",
      "detection IN text_01\n",
      "se agregó:  1.0 en:  0 64\n",
      "detection NOT IN text_02\n",
      "se agregó:  0.0 en:  1 64\n",
      "detection NOT IN text_03\n",
      "se agregó:  0.0 en:  2 64\n",
      "is IN text_01\n",
      "se agregó:  4.0 en:  0 65\n",
      "is IN text_02\n",
      "se agregó:  4.0 en:  1 65\n",
      "is NOT IN text_03\n",
      "se agregó:  0.0 en:  2 65\n",
      "huge IN text_01\n",
      "se agregó:  1.0 en:  0 66\n",
      "huge NOT IN text_02\n",
      "se agregó:  0.0 en:  1 66\n",
      "huge NOT IN text_03\n",
      "se agregó:  0.0 en:  2 66\n",
      "list IN text_01\n",
      "se agregó:  1.0 en:  0 67\n",
      "list NOT IN text_02\n",
      "se agregó:  0.0 en:  1 67\n",
      "list NOT IN text_03\n",
      "se agregó:  0.0 en:  2 67\n",
      "things IN text_01\n",
      "se agregó:  1.0 en:  0 68\n",
      "things NOT IN text_02\n",
      "se agregó:  0.0 en:  1 68\n",
      "things NOT IN text_03\n",
      "se agregó:  0.0 en:  2 68\n",
      "d IN text_01\n",
      "se agregó:  1.0 en:  0 69\n",
      "d NOT IN text_02\n",
      "se agregó:  0.0 en:  1 69\n",
      "d NOT IN text_03\n",
      "se agregó:  0.0 en:  2 69\n",
      "optimally IN text_01\n",
      "se agregó:  1.0 en:  0 70\n",
      "optimally NOT IN text_02\n",
      "se agregó:  0.0 en:  1 70\n",
      "optimally NOT IN text_03\n",
      "se agregó:  0.0 en:  2 70\n",
      "like IN text_01\n",
      "se agregó:  1.0 en:  0 71\n",
      "like NOT IN text_02\n",
      "se agregó:  0.0 en:  1 71\n",
      "like NOT IN text_03\n",
      "se agregó:  0.0 en:  2 71\n",
      "it IN text_01\n",
      "se agregó:  2.0 en:  0 72\n",
      "it IN text_02\n",
      "se agregó:  2.0 en:  1 72\n",
      "it NOT IN text_03\n",
      "se agregó:  0.0 en:  2 72\n",
      "there NOT IN text_01\n",
      "se agregó:  0.0 en:  0 73\n",
      "there IN text_02\n",
      "se agregó:  1.0 en:  1 73\n",
      "there NOT IN text_03\n",
      "se agregó:  0.0 en:  2 73\n",
      "caveat NOT IN text_01\n",
      "se agregó:  0.0 en:  0 74\n",
      "caveat IN text_02\n",
      "se agregó:  1.0 en:  1 74\n",
      "caveat NOT IN text_03\n",
      "se agregó:  0.0 en:  2 74\n",
      "course NOT IN text_01\n",
      "se agregó:  0.0 en:  0 75\n",
      "course IN text_02\n",
      "se agregó:  1.0 en:  1 75\n",
      "course NOT IN text_03\n",
      "se agregó:  0.0 en:  2 75\n",
      "while NOT IN text_01\n",
      "se agregó:  0.0 en:  0 76\n",
      "while IN text_02\n",
      "se agregó:  1.0 en:  1 76\n",
      "while NOT IN text_03\n",
      "se agregó:  0.0 en:  2 76\n",
      "full NOT IN text_01\n",
      "se agregó:  0.0 en:  0 77\n",
      "full IN text_02\n",
      "se agregó:  1.0 en:  1 77\n",
      "full NOT IN text_03\n",
      "se agregó:  0.0 en:  2 77\n",
      "automation NOT IN text_01\n",
      "se agregó:  0.0 en:  0 78\n",
      "automation IN text_02\n",
      "se agregó:  2.0 en:  1 78\n",
      "automation NOT IN text_03\n",
      "se agregó:  0.0 en:  2 78\n",
      "can NOT IN text_01\n",
      "se agregó:  0.0 en:  0 79\n",
      "can IN text_02\n",
      "se agregó:  1.0 en:  1 79\n",
      "can NOT IN text_03\n",
      "se agregó:  0.0 en:  2 79\n",
      "motivate NOT IN text_01\n",
      "se agregó:  0.0 en:  0 80\n",
      "motivate IN text_02\n",
      "se agregó:  1.0 en:  1 80\n",
      "motivate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 80\n",
      "scientific NOT IN text_01\n",
      "se agregó:  0.0 en:  0 81\n",
      "scientific IN text_02\n",
      "se agregó:  1.0 en:  1 81\n",
      "scientific NOT IN text_03\n",
      "se agregó:  0.0 en:  2 81\n",
      "research NOT IN text_01\n",
      "se agregó:  0.0 en:  0 82\n",
      "research IN text_02\n",
      "se agregó:  1.0 en:  1 82\n",
      "research NOT IN text_03\n",
      "se agregó:  0.0 en:  2 82\n",
      "provide NOT IN text_01\n",
      "se agregó:  0.0 en:  0 83\n",
      "provide IN text_02\n",
      "se agregó:  1.0 en:  1 83\n",
      "provide NOT IN text_03\n",
      "se agregó:  0.0 en:  2 83\n",
      "long NOT IN text_01\n",
      "se agregó:  0.0 en:  0 84\n",
      "long IN text_02\n",
      "se agregó:  1.0 en:  1 84\n",
      "long NOT IN text_03\n",
      "se agregó:  0.0 en:  2 84\n",
      "term NOT IN text_01\n",
      "se agregó:  0.0 en:  0 85\n",
      "term IN text_02\n",
      "se agregó:  1.0 en:  1 85\n",
      "term NOT IN text_03\n",
      "se agregó:  0.0 en:  2 85\n",
      "engineering NOT IN text_01\n",
      "se agregó:  0.0 en:  0 86\n",
      "engineering IN text_02\n",
      "se agregó:  1.0 en:  1 86\n",
      "engineering NOT IN text_03\n",
      "se agregó:  0.0 en:  2 86\n",
      "goal NOT IN text_01\n",
      "se agregó:  0.0 en:  0 87\n",
      "goal IN text_02\n",
      "se agregó:  1.0 en:  1 87\n",
      "goal NOT IN text_03\n",
      "se agregó:  0.0 en:  2 87\n",
      "practice NOT IN text_01\n",
      "se agregó:  0.0 en:  0 88\n",
      "practice IN text_02\n",
      "se agregó:  2.0 en:  1 88\n",
      "practice NOT IN text_03\n",
      "se agregó:  0.0 en:  2 88\n",
      "probably NOT IN text_01\n",
      "se agregó:  0.0 en:  0 89\n",
      "probably IN text_02\n",
      "se agregó:  1.0 en:  1 89\n",
      "probably NOT IN text_03\n",
      "se agregó:  0.0 en:  2 89\n",
      "semiautomate NOT IN text_01\n",
      "se agregó:  0.0 en:  0 90\n",
      "semiautomate IN text_02\n",
      "se agregó:  1.0 en:  1 90\n",
      "semiautomate NOT IN text_03\n",
      "se agregó:  0.0 en:  2 90\n",
      "most NOT IN text_01\n",
      "se agregó:  0.0 en:  0 91\n",
      "most IN text_02\n",
      "se agregó:  1.0 en:  1 91\n",
      "most NOT IN text_03\n",
      "se agregó:  0.0 en:  2 91\n",
      "these NOT IN text_01\n",
      "se agregó:  0.0 en:  0 92\n",
      "these IN text_02\n",
      "se agregó:  2.0 en:  1 92\n",
      "these NOT IN text_03\n",
      "se agregó:  0.0 en:  2 92\n",
      "gradually NOT IN text_01\n",
      "se agregó:  0.0 en:  0 93\n",
      "gradually IN text_02\n",
      "se agregó:  1.0 en:  1 93\n",
      "gradually NOT IN text_03\n",
      "se agregó:  0.0 en:  2 93\n",
      "remove NOT IN text_01\n",
      "se agregó:  0.0 en:  0 94\n",
      "remove IN text_02\n",
      "se agregó:  1.0 en:  1 94\n",
      "remove NOT IN text_03\n",
      "se agregó:  0.0 en:  2 94\n",
      "human NOT IN text_01\n",
      "se agregó:  0.0 en:  0 95\n",
      "human IN text_02\n",
      "se agregó:  1.0 en:  1 95\n",
      "human NOT IN text_03\n",
      "se agregó:  0.0 en:  2 95\n",
      "loop NOT IN text_01\n",
      "se agregó:  0.0 en:  0 96\n",
      "loop IN text_02\n",
      "se agregó:  1.0 en:  1 96\n",
      "loop NOT IN text_03\n",
      "se agregó:  0.0 en:  2 96\n",
      "as NOT IN text_01\n",
      "se agregó:  0.0 en:  0 97\n",
      "as IN text_02\n",
      "se agregó:  1.0 en:  1 97\n",
      "as NOT IN text_03\n",
      "se agregó:  0.0 en:  2 97\n",
      "needed NOT IN text_01\n",
      "se agregó:  0.0 en:  0 98\n",
      "needed IN text_02\n",
      "se agregó:  1.0 en:  1 98\n",
      "needed NOT IN text_03\n",
      "se agregó:  0.0 en:  2 98\n",
      "along NOT IN text_01\n",
      "se agregó:  0.0 en:  0 99\n",
      "along IN text_02\n",
      "se agregó:  1.0 en:  1 99\n",
      "along NOT IN text_03\n",
      "se agregó:  0.0 en:  2 99\n",
      "way NOT IN text_01\n",
      "se agregó:  0.0 en:  0 100\n",
      "way IN text_02\n",
      "se agregó:  1.0 en:  1 100\n",
      "way NOT IN text_03\n",
      "se agregó:  0.0 en:  2 100\n",
      "what NOT IN text_01\n",
      "se agregó:  0.0 en:  0 101\n",
      "what IN text_02\n",
      "se agregó:  1.0 en:  1 101\n",
      "what NOT IN text_03\n",
      "se agregó:  0.0 en:  2 101\n",
      "going NOT IN text_01\n",
      "se agregó:  0.0 en:  0 102\n",
      "going IN text_02\n",
      "se agregó:  1.0 en:  1 102\n",
      "going NOT IN text_03\n",
      "se agregó:  0.0 en:  2 102\n",
      "happen NOT IN text_01\n",
      "se agregó:  0.0 en:  0 103\n",
      "happen IN text_02\n",
      "se agregó:  1.0 en:  1 103\n",
      "happen NOT IN text_03\n",
      "se agregó:  0.0 en:  2 103\n",
      "if NOT IN text_01\n",
      "se agregó:  0.0 en:  0 104\n",
      "if IN text_02\n",
      "se agregó:  1.0 en:  1 104\n",
      "if NOT IN text_03\n",
      "se agregó:  0.0 en:  2 104\n",
      "do NOT IN text_01\n",
      "se agregó:  0.0 en:  0 105\n",
      "do IN text_02\n",
      "se agregó:  1.0 en:  1 105\n",
      "do NOT IN text_03\n",
      "se agregó:  0.0 en:  2 105\n",
      "that NOT IN text_01\n",
      "se agregó:  0.0 en:  0 106\n",
      "that IN text_02\n",
      "se agregó:  2.0 en:  1 106\n",
      "that NOT IN text_03\n",
      "se agregó:  0.0 en:  2 106\n",
      "are NOT IN text_01\n",
      "se agregó:  0.0 en:  0 107\n",
      "are IN text_02\n",
      "se agregó:  1.0 en:  1 107\n",
      "are NOT IN text_03\n",
      "se agregó:  0.0 en:  2 107\n",
      "likely NOT IN text_01\n",
      "se agregó:  0.0 en:  0 108\n",
      "likely IN text_02\n",
      "se agregó:  1.0 en:  1 108\n",
      "likely NOT IN text_03\n",
      "se agregó:  0.0 en:  2 108\n",
      "develop NOT IN text_01\n",
      "se agregó:  0.0 en:  0 109\n",
      "develop IN text_02\n",
      "se agregó:  1.0 en:  1 109\n",
      "develop NOT IN text_03\n",
      "se agregó:  0.0 en:  2 109\n",
      "powerful NOT IN text_01\n",
      "se agregó:  0.0 en:  0 110\n",
      "powerful IN text_02\n",
      "se agregó:  1.0 en:  1 110\n",
      "powerful NOT IN text_03\n",
      "se agregó:  0.0 en:  2 110\n",
      "tools NOT IN text_01\n",
      "se agregó:  0.0 en:  0 111\n",
      "tools IN text_02\n",
      "se agregó:  1.0 en:  1 111\n",
      "tools NOT IN text_03\n",
      "se agregó:  0.0 en:  2 111\n",
      "will NOT IN text_01\n",
      "se agregó:  0.0 en:  0 112\n",
      "will IN text_02\n",
      "se agregó:  1.0 en:  1 112\n",
      "will NOT IN text_03\n",
      "se agregó:  0.0 en:  2 112\n",
      "help NOT IN text_01\n",
      "se agregó:  0.0 en:  0 113\n",
      "help IN text_02\n",
      "se agregó:  1.0 en:  1 113\n",
      "help NOT IN text_03\n",
      "se agregó:  0.0 en:  2 113\n",
      "make NOT IN text_01\n",
      "se agregó:  0.0 en:  0 114\n",
      "make IN text_02\n",
      "se agregó:  1.0 en:  1 114\n",
      "make NOT IN text_03\n",
      "se agregó:  0.0 en:  2 114\n",
      "first NOT IN text_01\n",
      "se agregó:  0.0 en:  0 115\n",
      "first IN text_02\n",
      "se agregó:  1.0 en:  1 115\n",
      "first NOT IN text_03\n",
      "se agregó:  0.0 en:  2 115\n",
      "more NOT IN text_01\n",
      "se agregó:  0.0 en:  0 116\n",
      "more IN text_02\n",
      "se agregó:  2.0 en:  1 116\n",
      "more NOT IN text_03\n",
      "se agregó:  0.0 en:  2 116\n",
      "systematic NOT IN text_01\n",
      "se agregó:  0.0 en:  0 117\n",
      "systematic IN text_02\n",
      "se agregó:  1.0 en:  1 117\n",
      "systematic NOT IN text_03\n",
      "se agregó:  0.0 en:  2 117\n",
      "s NOT IN text_01\n",
      "se agregó:  0.0 en:  0 118\n",
      "s IN text_02\n",
      "se agregó:  1.0 en:  1 118\n",
      "s NOT IN text_03\n",
      "se agregó:  0.0 en:  2 118\n",
      "ad NOT IN text_01\n",
      "se agregó:  0.0 en:  0 119\n",
      "ad IN text_02\n",
      "se agregó:  1.0 en:  1 119\n",
      "ad NOT IN text_03\n",
      "se agregó:  0.0 en:  2 119\n",
      "hoc NOT IN text_01\n",
      "se agregó:  0.0 en:  0 120\n",
      "hoc IN text_02\n",
      "se agregó:  1.0 en:  1 120\n",
      "hoc NOT IN text_03\n",
      "se agregó:  0.0 en:  2 120\n",
      "days NOT IN text_01\n",
      "se agregó:  0.0 en:  0 121\n",
      "days IN text_02\n",
      "se agregó:  1.0 en:  1 121\n",
      "days NOT IN text_03\n",
      "se agregó:  0.0 en:  2 121\n",
      "also NOT IN text_01\n",
      "se agregó:  0.0 en:  0 122\n",
      "also IN text_02\n",
      "se agregó:  1.0 en:  1 122\n",
      "also NOT IN text_03\n",
      "se agregó:  0.0 en:  2 122\n",
      "efficient NOT IN text_01\n",
      "se agregó:  0.0 en:  0 123\n",
      "efficient IN text_02\n",
      "se agregó:  1.0 en:  1 123\n",
      "efficient NOT IN text_03\n",
      "se agregó:  0.0 en:  2 123\n",
      "buenos NOT IN text_01\n",
      "se agregó:  0.0 en:  0 124\n",
      "buenos NOT IN text_02\n",
      "se agregó:  0.0 en:  1 124\n",
      "buenos IN text_03\n",
      "se agregó:  1.0 en:  2 124\n",
      "días NOT IN text_01\n",
      "se agregó:  0.0 en:  0 125\n",
      "días NOT IN text_02\n",
      "se agregó:  0.0 en:  1 125\n",
      "días IN text_03\n",
      "se agregó:  1.0 en:  2 125\n",
      "país NOT IN text_01\n",
      "se agregó:  0.0 en:  0 126\n",
      "país NOT IN text_02\n",
      "se agregó:  0.0 en:  1 126\n",
      "país IN text_03\n",
      "se agregó:  1.0 en:  2 126\n",
      "subsecretario NOT IN text_01\n",
      "se agregó:  0.0 en:  0 127\n",
      "subsecretario NOT IN text_02\n",
      "se agregó:  0.0 en:  1 127\n",
      "subsecretario IN text_03\n",
      "se agregó:  1.0 en:  2 127\n",
      "educación NOT IN text_01\n",
      "se agregó:  0.0 en:  0 128\n",
      "educación NOT IN text_02\n",
      "se agregó:  0.0 en:  1 128\n",
      "educación IN text_03\n",
      "se agregó:  1.0 en:  2 128\n",
      "dice NOT IN text_01\n",
      "se agregó:  0.0 en:  0 129\n",
      "dice NOT IN text_02\n",
      "se agregó:  0.0 en:  1 129\n",
      "dice IN text_03\n",
      "se agregó:  1.0 en:  2 129\n",
      "comunismo NOT IN text_01\n",
      "se agregó:  0.0 en:  0 130\n",
      "comunismo NOT IN text_02\n",
      "se agregó:  0.0 en:  1 130\n",
      "comunismo IN text_03\n",
      "se agregó:  1.0 en:  2 130\n",
      "necesario NOT IN text_01\n",
      "se agregó:  0.0 en:  0 131\n",
      "necesario NOT IN text_02\n",
      "se agregó:  0.0 en:  1 131\n",
      "necesario IN text_03\n",
      "se agregó:  1.0 en:  2 131\n",
      "transformar NOT IN text_01\n",
      "se agregó:  0.0 en:  0 132\n",
      "transformar NOT IN text_02\n",
      "se agregó:  0.0 en:  1 132\n",
      "transformar IN text_03\n",
      "se agregó:  1.0 en:  2 132\n",
      "méxico NOT IN text_01\n",
      "se agregó:  0.0 en:  0 133\n",
      "méxico NOT IN text_02\n",
      "se agregó:  0.0 en:  1 133\n",
      "méxico IN text_03\n",
      "se agregó:  1.0 en:  2 133\n",
      "disparan NOT IN text_01\n",
      "se agregó:  0.0 en:  0 134\n",
      "disparan NOT IN text_02\n",
      "se agregó:  0.0 en:  1 134\n",
      "disparan IN text_03\n",
      "se agregó:  1.0 en:  2 134\n",
      "312 NOT IN text_01\n",
      "se agregó:  0.0 en:  0 135\n",
      "312 NOT IN text_02\n",
      "se agregó:  0.0 en:  1 135\n",
      "312 IN text_03\n",
      "se agregó:  1.0 en:  2 135\n",
      "casos NOT IN text_01\n",
      "se agregó:  0.0 en:  0 136\n",
      "casos NOT IN text_02\n",
      "se agregó:  0.0 en:  1 136\n",
      "casos IN text_03\n",
      "se agregó:  1.0 en:  2 136\n",
      "dengue NOT IN text_01\n",
      "se agregó:  0.0 en:  0 137\n",
      "dengue NOT IN text_02\n",
      "se agregó:  0.0 en:  1 137\n",
      "dengue IN text_03\n",
      "se agregó:  1.0 en:  2 137\n",
      "compraron NOT IN text_01\n",
      "se agregó:  0.0 en:  0 138\n",
      "compraron NOT IN text_02\n",
      "se agregó:  0.0 en:  1 138\n",
      "compraron IN text_03\n",
      "se agregó:  1.0 en:  2 138\n",
      "insecticidas NOT IN text_01\n",
      "se agregó:  0.0 en:  0 139\n",
      "insecticidas NOT IN text_02\n",
      "se agregó:  0.0 en:  1 139\n",
      "insecticidas IN text_03\n",
      "se agregó:  1.0 en:  2 139\n",
      "mireles NOT IN text_01\n",
      "se agregó:  0.0 en:  0 140\n",
      "mireles NOT IN text_02\n",
      "se agregó:  0.0 en:  1 140\n",
      "mireles IN text_03\n",
      "se agregó:  1.0 en:  2 140\n",
      "llama NOT IN text_01\n",
      "se agregó:  0.0 en:  0 141\n",
      "llama NOT IN text_02\n",
      "se agregó:  0.0 en:  1 141\n",
      "llama IN text_03\n",
      "se agregó:  1.0 en:  2 141\n",
      "pirujas NOT IN text_01\n",
      "se agregó:  0.0 en:  0 142\n",
      "pirujas NOT IN text_02\n",
      "se agregó:  0.0 en:  1 142\n",
      "pirujas IN text_03\n",
      "se agregó:  1.0 en:  2 142\n",
      "concubinas NOT IN text_01\n",
      "se agregó:  0.0 en:  0 143\n",
      "concubinas NOT IN text_02\n",
      "se agregó:  0.0 en:  1 143\n",
      "concubinas IN text_03\n",
      "se agregó:  1.0 en:  2 143\n",
      "presi NOT IN text_01\n",
      "se agregó:  0.0 en:  0 144\n",
      "presi NOT IN text_02\n",
      "se agregó:  0.0 en:  1 144\n",
      "presi IN text_03\n",
      "se agregó:  1.0 en:  2 144\n",
      "quiere NOT IN text_01\n",
      "se agregó:  0.0 en:  0 145\n",
      "quiere NOT IN text_02\n",
      "se agregó:  0.0 en:  1 145\n",
      "quiere IN text_03\n",
      "se agregó:  1.0 en:  2 145\n",
      "quitar NOT IN text_01\n",
      "se agregó:  0.0 en:  0 146\n",
      "quitar NOT IN text_02\n",
      "se agregó:  0.0 en:  1 146\n",
      "quitar IN text_03\n",
      "se agregó:  1.0 en:  2 146\n",
      "ine NOT IN text_01\n",
      "se agregó:  0.0 en:  0 147\n",
      "ine NOT IN text_02\n",
      "se agregó:  0.0 en:  1 147\n",
      "ine IN text_03\n",
      "se agregó:  1.0 en:  2 147\n",
      "café NOT IN text_01\n",
      "se agregó:  0.0 en:  0 148\n",
      "café NOT IN text_02\n",
      "se agregó:  0.0 en:  1 148\n",
      "café IN text_03\n",
      "se agregó:  1.0 en:  2 148\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "\n",
    "for word_termns in dicc_termns: #dicc_termns todos los términos\n",
    "#    print()\n",
    "    for word_texts in dicc_texts: #dicc_texts todos los textos\n",
    "#        print(\"EVALUAR:\", word_termns, \"EN: \", word_texts)\n",
    "        if(word_termns in dicc_texts[word_texts]): #si está\n",
    "            print(word_termns, \"IN\", word_texts)\n",
    "            \n",
    "            matrix[j, i] = dicc_termns[word_termns]\n",
    "            \n",
    "        elif(word_termns not in dicc_texts[word_texts]): # si no está\n",
    "            print(word_termns, \"NOT IN\", word_texts)\n",
    "            \n",
    "            matrix[j, i] = 0\n",
    "            \n",
    "            \n",
    "        print(\"se agregó: \", matrix[j,i], \"en: \", j, i)\n",
    "            \n",
    "        j = j + 1\n",
    "        \n",
    "    j = 0\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  1.,  1.,  2.,  1.,  1.,  9.,  3.,  3.,  1.,  1.,  2.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  4.,  1.,  2.,  1.,  7.,  1.,  1.,  1.,\n",
       "         4.,  1.,  5.,  1.,  2.,  2.,  4.,  1.,  8.,  6.,  1., 11.,  4.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         4.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  2.,  0.,  0.,  0.,  3.,  3.,  0.,  0.,  2.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  4.,  0.,  2.,  0.,  7.,  0.,  0.,  0.,\n",
       "         4.,  0.,  5.,  0.,  2.,  0.,  4.,  0.,  8.,  6.,  0., 11.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         4.,  0.,  0.,  0.,  0.,  0.,  0.,  2.,  1.,  1.,  1.,  1.,  1.,\n",
       "         2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,  1.,  1.,\n",
       "         1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  2.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  2.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,\n",
       "         1.,  1.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 149)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#matrix[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7810573287503588"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cos_t01_t02 = dot(matrix[0],matrix[1])/(norm(matrix[0])*norm(matrix[1]))\n",
    "df_cos_t01_t02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cos_t02_t03 = dot(matrix[1],matrix[2])/(norm(matrix[1])*norm(matrix[2]))\n",
    "df_cos_t02_t03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cos_t01_t03 = dot(matrix[0],matrix[2])/(norm(matrix[0])*norm(matrix[2]))\n",
    "df_cos_t01_t03"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
